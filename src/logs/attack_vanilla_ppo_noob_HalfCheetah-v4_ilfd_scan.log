No ROCm runtime is found, using ROCM_HOME='/gs/bs/tga-mdl/yamabe-mdl/implement/SAIA/src/Usage:
/usr/bin/which.debianutils
/gs/bs/tga-mdl/yamabe-mdl/implement/SAIA/src/[-a]
/gs/bs/tga-mdl/yamabe-mdl/implement/SAIA'
No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'
Job split into 1 part, this is part 1
Total 9 jobs. Job start 1, end 9
We will handle 9 jobs.
python run.py --config-path ../configs/agent_configs_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan/000.json
python run.py --config-path ../configs/agent_configs_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan/001.json
python run.py --config-path ../configs/agent_configs_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan/002.json
python run.py --config-path ../configs/agent_configs_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan/003.json
python run.py --config-path ../configs/agent_configs_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan/004.json
python run.py --config-path ../configs/agent_configs_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan/005.json
python run.py --config-path ../configs/agent_configs_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan/006.json
python run.py --config-path ../configs/agent_configs_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan/007.json
python run.py --config-path ../configs/agent_configs_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan/008.json
thread 3 using cpu [3, 195]
disabling policy training (train adversary only)
Running config: {'action_noise_std': 0.1, 'adam_eps': 1e-05, 'adv_adversary_ratio': 1.0, 'adv_adversary_steps': 1, 'adv_clip_eps': 0.2, 'adv_disc_lr': 0.0001, 'adv_entropy_coeff': 1e-05, 'adv_eps': 'same', 'adv_policy_steps': 1, 'adv_ppo_lr_adam': 0.0003, 'adv_q_lr': 'same', 'adv_q_net_type': 'DoubleQNet', 'adv_val_lr': 0.0001, 'advanced_logging': False, 'anneal_lr': True, 'attack_advpolicy_network': 'advpolicy.model', 'attack_eps': 'same', 'attack_method': 'none', 'attack_ratio': 1.0, 'attack_sarsa_action_ratio': 0.5, 'attack_sarsa_network': 'sarsa.model', 'attack_step_eps': 'auto', 'attack_steps': 10, 'batch_size': 256, 'buffer_size': 1000000, 'cg_steps': 10, 'clip_eps': 0.2, 'clip_grad_norm': -1, 'clip_observations': 10.0, 'clip_rewards': 10.0, 'clip_val_eps': 0.2, 'collect_perturbed_states': True, 'cpu': True, 'damping': 0.1, 'demo_episode_size': 20, 'disc_gradient_steps': 1, 'entropy_coeff': 0.0, 'expert_buffer_path': 'demo/Ant-v4_demo.pkl', 'fisher_frac_samples': 0.1, 'force_stop_step': -1, 'game': 'HalfCheetah-v4', 'gamma': 0.99, 'gp_lambda': 10.0, 'gradient_steps': 250, 'history_length': -1, 'initial_std': 1.0, 'initialization': 'orthogonal', 'kl_approximation_iters': -1, 'lambda': 0.95, 'learning_starts': 10000, 'load_model': 'models/vanilla_ppo_noob_HalfCheetah-v4.model', 'log_every': 1, 'max_backtrack': 10, 'max_kl': 0.01, 'max_kl_final': 0.01, 'mode': 'adv_ilfd', 'noise_clip': 0.3, 'norm_rewards': 'none', 'norm_states': True, 'num_actors': 1, 'num_minibatches': 32, 'out_dir': '../configs/agents_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan/006', 'policy_activation': 'tanh', 'policy_delay': 2, 'policy_net_type': 'CtsPolicyTD3', 'ppo_epochs': 10, 'ppo_lr': -1, 'ppo_lr_adam': 0.0, 'robust_ppo_beta': 1.0, 'robust_ppo_beta_scheduler_opts': 'same', 'robust_ppo_detach_stdev': False, 'robust_ppo_eps': 0.3, 'robust_ppo_eps_scheduler_opts': 'start=1,length=732', 'robust_ppo_method': 'convex-relax', 'robust_ppo_pgd_steps': 10, 'robust_ppo_reg': 0.1, 'save_frames': False, 'save_frames_path': 'frames/', 'save_iters': 1000, 'save_normalization_factor': False, 'share_weights': False, 'show_env': False, 't': 1000, 'tau': 0.005, 'train_steps': 2400, 'trpo_kl_reduce_func': 'mean', 'use_lstm_val': True, 'val_epochs': 10, 'val_lr': 0.00015, 'value_calc': 'gae', 'value_clipping': True, 'value_multiplier': 0.1, 'value_net_type': 'ValueNet', 'config_path': ['../configs/agent_configs_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan'], 'out_dir_prefix': '../configs/agents_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan', 'num_splits': 1, 'index': 1, 'start': -1, 'end': -1, 'ncpus': 1, 'start_cpuid': 0, 'adv_policy_only': True, 'deterministic': True, 'no_load_adv_policy': False, 'no_smt': False}
Logging in: /gs/bs/tga-mdl/yamabe-mdl/implement/SAIA/configs/agents_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan/006/211a5f9f-b54f-463e-86d5-3828b23c2510
Load normalization factors from normalized_envs/HalfCheetah-v4.pkl
Using activation function ReLU()
Using activation function ReLU()
Using activation function ReLU()
Loading pretrained model models/vanilla_ppo_noob_HalfCheetah-v4.model
Policy runs in deterministic mode. Ignoring Gaussian noise.
Step 0
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -277.580454 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 1000 / 10000
policy lr: [0.000299875], q lr: [0.000299875], disc lr: [9.995833333333334e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.8778984546661377
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------
thread 6 using cpu [6, 198]
disabling policy training (train adversary only)
Running config: {'action_noise_std': 0.1, 'adam_eps': 1e-05, 'adv_adversary_ratio': 1.0, 'adv_adversary_steps': 1, 'adv_clip_eps': 0.2, 'adv_disc_lr': 0.0001, 'adv_entropy_coeff': 1e-05, 'adv_eps': 'same', 'adv_policy_steps': 1, 'adv_ppo_lr_adam': 0.0003, 'adv_q_lr': 'same', 'adv_q_net_type': 'DoubleQNet', 'adv_val_lr': 0.0001, 'advanced_logging': False, 'anneal_lr': True, 'attack_advpolicy_network': 'advpolicy.model', 'attack_eps': 'same', 'attack_method': 'none', 'attack_ratio': 1.0, 'attack_sarsa_action_ratio': 0.5, 'attack_sarsa_network': 'sarsa.model', 'attack_step_eps': 'auto', 'attack_steps': 10, 'batch_size': 256, 'buffer_size': 1000000, 'cg_steps': 10, 'clip_eps': 0.2, 'clip_grad_norm': -1, 'clip_observations': 10.0, 'clip_rewards': 10.0, 'clip_val_eps': 0.2, 'collect_perturbed_states': True, 'cpu': True, 'damping': 0.1, 'demo_episode_size': 20, 'disc_gradient_steps': 1, 'entropy_coeff': 0.0, 'expert_buffer_path': 'demo/Ant-v4_demo.pkl', 'fisher_frac_samples': 0.1, 'force_stop_step': -1, 'game': 'HalfCheetah-v4', 'gamma': 0.99, 'gp_lambda': 10.0, 'gradient_steps': 250, 'history_length': -1, 'initial_std': 1.0, 'initialization': 'orthogonal', 'kl_approximation_iters': -1, 'lambda': 0.95, 'learning_starts': 10000, 'load_model': 'models/vanilla_ppo_noob_HalfCheetah-v4.model', 'log_every': 1, 'max_backtrack': 10, 'max_kl': 0.01, 'max_kl_final': 0.01, 'mode': 'adv_ilfd', 'noise_clip': 0.3, 'norm_rewards': 'none', 'norm_states': True, 'num_actors': 1, 'num_minibatches': 32, 'out_dir': '../configs/agents_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan/003', 'policy_activation': 'tanh', 'policy_delay': 2, 'policy_net_type': 'CtsPolicyTD3', 'ppo_epochs': 10, 'ppo_lr': -1, 'ppo_lr_adam': 0.0, 'robust_ppo_beta': 1.0, 'robust_ppo_beta_scheduler_opts': 'same', 'robust_ppo_detach_stdev': False, 'robust_ppo_eps': 0.15, 'robust_ppo_eps_scheduler_opts': 'start=1,length=732', 'robust_ppo_method': 'convex-relax', 'robust_ppo_pgd_steps': 10, 'robust_ppo_reg': 0.1, 'save_frames': False, 'save_frames_path': 'frames/', 'save_iters': 1000, 'save_normalization_factor': False, 'share_weights': False, 'show_env': False, 't': 1000, 'tau': 0.005, 'train_steps': 2400, 'trpo_kl_reduce_func': 'mean', 'use_lstm_val': True, 'val_epochs': 10, 'val_lr': 0.00015, 'value_calc': 'gae', 'value_clipping': True, 'value_multiplier': 0.1, 'value_net_type': 'ValueNet', 'config_path': ['../configs/agent_configs_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan'], 'out_dir_prefix': '../configs/agents_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan', 'num_splits': 1, 'index': 1, 'start': -1, 'end': -1, 'ncpus': 1, 'start_cpuid': 0, 'adv_policy_only': True, 'deterministic': True, 'no_load_adv_policy': False, 'no_smt': False}
Logging in: /gs/bs/tga-mdl/yamabe-mdl/implement/SAIA/configs/agents_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan/003/0ec115a6-9af7-40b7-86d4-d5ebd8161152
Load normalization factors from normalized_envs/HalfCheetah-v4.pkl
Using activation function ReLU()
Using activation function ReLU()
Using activation function ReLU()
Loading pretrained model models/vanilla_ppo_noob_HalfCheetah-v4.model
Policy runs in deterministic mode. Ignoring Gaussian noise.
Step 0
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -285.521839 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 1000 / 10000
policy lr: [0.000299875], q lr: [0.000299875], disc lr: [9.995833333333334e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.8778848648071289
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------
thread 4 using cpu [4, 196]
disabling policy training (train adversary only)
Running config: {'action_noise_std': 0.1, 'adam_eps': 1e-05, 'adv_adversary_ratio': 1.0, 'adv_adversary_steps': 1, 'adv_clip_eps': 0.2, 'adv_disc_lr': 0.0001, 'adv_entropy_coeff': 1e-05, 'adv_eps': 'same', 'adv_policy_steps': 1, 'adv_ppo_lr_adam': 0.0003, 'adv_q_lr': 'same', 'adv_q_net_type': 'DoubleQNet', 'adv_val_lr': 0.0001, 'advanced_logging': False, 'anneal_lr': True, 'attack_advpolicy_network': 'advpolicy.model', 'attack_eps': 'same', 'attack_method': 'none', 'attack_ratio': 1.0, 'attack_sarsa_action_ratio': 0.5, 'attack_sarsa_network': 'sarsa.model', 'attack_step_eps': 'auto', 'attack_steps': 10, 'batch_size': 256, 'buffer_size': 1000000, 'cg_steps': 10, 'clip_eps': 0.2, 'clip_grad_norm': -1, 'clip_observations': 10.0, 'clip_rewards': 10.0, 'clip_val_eps': 0.2, 'collect_perturbed_states': True, 'cpu': True, 'damping': 0.1, 'demo_episode_size': 20, 'disc_gradient_steps': 1, 'entropy_coeff': 0.0, 'expert_buffer_path': 'demo/Ant-v4_demo.pkl', 'fisher_frac_samples': 0.1, 'force_stop_step': -1, 'game': 'HalfCheetah-v4', 'gamma': 0.99, 'gp_lambda': 10.0, 'gradient_steps': 250, 'history_length': -1, 'initial_std': 1.0, 'initialization': 'orthogonal', 'kl_approximation_iters': -1, 'lambda': 0.95, 'learning_starts': 10000, 'load_model': 'models/vanilla_ppo_noob_HalfCheetah-v4.model', 'log_every': 1, 'max_backtrack': 10, 'max_kl': 0.01, 'max_kl_final': 0.01, 'mode': 'adv_ilfd', 'noise_clip': 0.3, 'norm_rewards': 'none', 'norm_states': True, 'num_actors': 1, 'num_minibatches': 32, 'out_dir': '../configs/agents_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan/005', 'policy_activation': 'tanh', 'policy_delay': 2, 'policy_net_type': 'CtsPolicyTD3', 'ppo_epochs': 10, 'ppo_lr': -1, 'ppo_lr_adam': 0.0, 'robust_ppo_beta': 1.0, 'robust_ppo_beta_scheduler_opts': 'same', 'robust_ppo_detach_stdev': False, 'robust_ppo_eps': 0.25, 'robust_ppo_eps_scheduler_opts': 'start=1,length=732', 'robust_ppo_method': 'convex-relax', 'robust_ppo_pgd_steps': 10, 'robust_ppo_reg': 0.1, 'save_frames': False, 'save_frames_path': 'frames/', 'save_iters': 1000, 'save_normalization_factor': False, 'share_weights': False, 'show_env': False, 't': 1000, 'tau': 0.005, 'train_steps': 2400, 'trpo_kl_reduce_func': 'mean', 'use_lstm_val': True, 'val_epochs': 10, 'val_lr': 0.00015, 'value_calc': 'gae', 'value_clipping': True, 'value_multiplier': 0.1, 'value_net_type': 'ValueNet', 'config_path': ['../configs/agent_configs_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan'], 'out_dir_prefix': '../configs/agents_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan', 'num_splits': 1, 'index': 1, 'start': -1, 'end': -1, 'ncpus': 1, 'start_cpuid': 0, 'adv_policy_only': True, 'deterministic': True, 'no_load_adv_policy': False, 'no_smt': False}
Logging in: /gs/bs/tga-mdl/yamabe-mdl/implement/SAIA/configs/agents_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan/005/d9e5f8d7-1810-42cf-8930-bb950ad91469
Load normalization factors from normalized_envs/HalfCheetah-v4.pkl
Using activation function ReLU()
Using activation function ReLU()
Using activation function ReLU()
Loading pretrained model models/vanilla_ppo_noob_HalfCheetah-v4.model
Policy runs in deterministic mode. Ignoring Gaussian noise.
Step 0
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -282.235938 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 1000 / 10000
policy lr: [0.000299875], q lr: [0.000299875], disc lr: [9.995833333333334e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.8779017925262451
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------
thread 1 using cpu [1, 193]
disabling policy training (train adversary only)
Running config: {'action_noise_std': 0.1, 'adam_eps': 1e-05, 'adv_adversary_ratio': 1.0, 'adv_adversary_steps': 1, 'adv_clip_eps': 0.2, 'adv_disc_lr': 0.0001, 'adv_entropy_coeff': 1e-05, 'adv_eps': 'same', 'adv_policy_steps': 1, 'adv_ppo_lr_adam': 0.0003, 'adv_q_lr': 'same', 'adv_q_net_type': 'DoubleQNet', 'adv_val_lr': 0.0001, 'advanced_logging': False, 'anneal_lr': True, 'attack_advpolicy_network': 'advpolicy.model', 'attack_eps': 'same', 'attack_method': 'none', 'attack_ratio': 1.0, 'attack_sarsa_action_ratio': 0.5, 'attack_sarsa_network': 'sarsa.model', 'attack_step_eps': 'auto', 'attack_steps': 10, 'batch_size': 256, 'buffer_size': 1000000, 'cg_steps': 10, 'clip_eps': 0.2, 'clip_grad_norm': -1, 'clip_observations': 10.0, 'clip_rewards': 10.0, 'clip_val_eps': 0.2, 'collect_perturbed_states': True, 'cpu': True, 'damping': 0.1, 'demo_episode_size': 20, 'disc_gradient_steps': 1, 'entropy_coeff': 0.0, 'expert_buffer_path': 'demo/Ant-v4_demo.pkl', 'fisher_frac_samples': 0.1, 'force_stop_step': -1, 'game': 'HalfCheetah-v4', 'gamma': 0.99, 'gp_lambda': 10.0, 'gradient_steps': 250, 'history_length': -1, 'initial_std': 1.0, 'initialization': 'orthogonal', 'kl_approximation_iters': -1, 'lambda': 0.95, 'learning_starts': 10000, 'load_model': 'models/vanilla_ppo_noob_HalfCheetah-v4.model', 'log_every': 1, 'max_backtrack': 10, 'max_kl': 0.01, 'max_kl_final': 0.01, 'mode': 'adv_ilfd', 'noise_clip': 0.3, 'norm_rewards': 'none', 'norm_states': True, 'num_actors': 1, 'num_minibatches': 32, 'out_dir': '../configs/agents_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan/004', 'policy_activation': 'tanh', 'policy_delay': 2, 'policy_net_type': 'CtsPolicyTD3', 'ppo_epochs': 10, 'ppo_lr': -1, 'ppo_lr_adam': 0.0, 'robust_ppo_beta': 1.0, 'robust_ppo_beta_scheduler_opts': 'same', 'robust_ppo_detach_stdev': False, 'robust_ppo_eps': 0.2, 'robust_ppo_eps_scheduler_opts': 'start=1,length=732', 'robust_ppo_method': 'convex-relax', 'robust_ppo_pgd_steps': 10, 'robust_ppo_reg': 0.1, 'save_frames': False, 'save_frames_path': 'frames/', 'save_iters': 1000, 'save_normalization_factor': False, 'share_weights': False, 'show_env': False, 't': 1000, 'tau': 0.005, 'train_steps': 2400, 'trpo_kl_reduce_func': 'mean', 'use_lstm_val': True, 'val_epochs': 10, 'val_lr': 0.00015, 'value_calc': 'gae', 'value_clipping': True, 'value_multiplier': 0.1, 'value_net_type': 'ValueNet', 'config_path': ['../configs/agent_configs_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan'], 'out_dir_prefix': '../configs/agents_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan', 'num_splits': 1, 'index': 1, 'start': -1, 'end': -1, 'ncpus': 1, 'start_cpuid': 0, 'adv_policy_only': True, 'deterministic': True, 'no_load_adv_policy': False, 'no_smt': False}
Logging in: /gs/bs/tga-mdl/yamabe-mdl/implement/SAIA/configs/agents_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan/004/002a87a7-34ac-4bdd-9fdf-9439d404adb4
Load normalization factors from normalized_envs/HalfCheetah-v4.pkl
Using activation function ReLU()
Using activation function ReLU()
Using activation function ReLU()
Loading pretrained model models/vanilla_ppo_noob_HalfCheetah-v4.model
Policy runs in deterministic mode. Ignoring Gaussian noise.
Step 0
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -281.892402 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 1000 / 10000
policy lr: [0.000299875], q lr: [0.000299875], disc lr: [9.995833333333334e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.8778741359710693
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------
thread 7 using cpu [7, 199]
disabling policy training (train adversary only)
Running config: {'action_noise_std': 0.1, 'adam_eps': 1e-05, 'adv_adversary_ratio': 1.0, 'adv_adversary_steps': 1, 'adv_clip_eps': 0.2, 'adv_disc_lr': 0.0001, 'adv_entropy_coeff': 1e-05, 'adv_eps': 'same', 'adv_policy_steps': 1, 'adv_ppo_lr_adam': 0.0003, 'adv_q_lr': 'same', 'adv_q_net_type': 'DoubleQNet', 'adv_val_lr': 0.0001, 'advanced_logging': False, 'anneal_lr': True, 'attack_advpolicy_network': 'advpolicy.model', 'attack_eps': 'same', 'attack_method': 'none', 'attack_ratio': 1.0, 'attack_sarsa_action_ratio': 0.5, 'attack_sarsa_network': 'sarsa.model', 'attack_step_eps': 'auto', 'attack_steps': 10, 'batch_size': 256, 'buffer_size': 1000000, 'cg_steps': 10, 'clip_eps': 0.2, 'clip_grad_norm': -1, 'clip_observations': 10.0, 'clip_rewards': 10.0, 'clip_val_eps': 0.2, 'collect_perturbed_states': True, 'cpu': True, 'damping': 0.1, 'demo_episode_size': 20, 'disc_gradient_steps': 1, 'entropy_coeff': 0.0, 'expert_buffer_path': 'demo/Ant-v4_demo.pkl', 'fisher_frac_samples': 0.1, 'force_stop_step': -1, 'game': 'HalfCheetah-v4', 'gamma': 0.99, 'gp_lambda': 10.0, 'gradient_steps': 250, 'history_length': -1, 'initial_std': 1.0, 'initialization': 'orthogonal', 'kl_approximation_iters': -1, 'lambda': 0.95, 'learning_starts': 10000, 'load_model': 'models/vanilla_ppo_noob_HalfCheetah-v4.model', 'log_every': 1, 'max_backtrack': 10, 'max_kl': 0.01, 'max_kl_final': 0.01, 'mode': 'adv_ilfd', 'noise_clip': 0.3, 'norm_rewards': 'none', 'norm_states': True, 'num_actors': 1, 'num_minibatches': 32, 'out_dir': '../configs/agents_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan/007', 'policy_activation': 'tanh', 'policy_delay': 2, 'policy_net_type': 'CtsPolicyTD3', 'ppo_epochs': 10, 'ppo_lr': -1, 'ppo_lr_adam': 0.0, 'robust_ppo_beta': 1.0, 'robust_ppo_beta_scheduler_opts': 'same', 'robust_ppo_detach_stdev': False, 'robust_ppo_eps': 0.4, 'robust_ppo_eps_scheduler_opts': 'start=1,length=732', 'robust_ppo_method': 'convex-relax', 'robust_ppo_pgd_steps': 10, 'robust_ppo_reg': 0.1, 'save_frames': False, 'save_frames_path': 'frames/', 'save_iters': 1000, 'save_normalization_factor': False, 'share_weights': False, 'show_env': False, 't': 1000, 'tau': 0.005, 'train_steps': 2400, 'trpo_kl_reduce_func': 'mean', 'use_lstm_val': True, 'val_epochs': 10, 'val_lr': 0.00015, 'value_calc': 'gae', 'value_clipping': True, 'value_multiplier': 0.1, 'value_net_type': 'ValueNet', 'config_path': ['../configs/agent_configs_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan'], 'out_dir_prefix': '../configs/agents_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan', 'num_splits': 1, 'index': 1, 'start': -1, 'end': -1, 'ncpus': 1, 'start_cpuid': 0, 'adv_policy_only': True, 'deterministic': True, 'no_load_adv_policy': False, 'no_smt': False}
Logging in: /gs/bs/tga-mdl/yamabe-mdl/implement/SAIA/configs/agents_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan/007/072af073-c27e-4026-950c-1e9bb9fb7cb5
Load normalization factors from normalized_envs/HalfCheetah-v4.pkl
Using activation function ReLU()
Using activation function ReLU()
Using activation function ReLU()
Loading pretrained model models/vanilla_ppo_noob_HalfCheetah-v4.model
Policy runs in deterministic mode. Ignoring Gaussian noise.
Step 0
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -278.597434 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 1000 / 10000
policy lr: [0.000299875], q lr: [0.000299875], disc lr: [9.995833333333334e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.8780508041381836
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------
thread 0 using cpu [0, 192]
disabling policy training (train adversary only)
Running config: {'action_noise_std': 0.1, 'adam_eps': 1e-05, 'adv_adversary_ratio': 1.0, 'adv_adversary_steps': 1, 'adv_clip_eps': 0.2, 'adv_disc_lr': 0.0001, 'adv_entropy_coeff': 1e-05, 'adv_eps': 'same', 'adv_policy_steps': 1, 'adv_ppo_lr_adam': 0.0003, 'adv_q_lr': 'same', 'adv_q_net_type': 'DoubleQNet', 'adv_val_lr': 0.0001, 'advanced_logging': False, 'anneal_lr': True, 'attack_advpolicy_network': 'advpolicy.model', 'attack_eps': 'same', 'attack_method': 'none', 'attack_ratio': 1.0, 'attack_sarsa_action_ratio': 0.5, 'attack_sarsa_network': 'sarsa.model', 'attack_step_eps': 'auto', 'attack_steps': 10, 'batch_size': 256, 'buffer_size': 1000000, 'cg_steps': 10, 'clip_eps': 0.2, 'clip_grad_norm': -1, 'clip_observations': 10.0, 'clip_rewards': 10.0, 'clip_val_eps': 0.2, 'collect_perturbed_states': True, 'cpu': True, 'damping': 0.1, 'demo_episode_size': 20, 'disc_gradient_steps': 1, 'entropy_coeff': 0.0, 'expert_buffer_path': 'demo/Ant-v4_demo.pkl', 'fisher_frac_samples': 0.1, 'force_stop_step': -1, 'game': 'HalfCheetah-v4', 'gamma': 0.99, 'gp_lambda': 10.0, 'gradient_steps': 250, 'history_length': -1, 'initial_std': 1.0, 'initialization': 'orthogonal', 'kl_approximation_iters': -1, 'lambda': 0.95, 'learning_starts': 10000, 'load_model': 'models/vanilla_ppo_noob_HalfCheetah-v4.model', 'log_every': 1, 'max_backtrack': 10, 'max_kl': 0.01, 'max_kl_final': 0.01, 'mode': 'adv_ilfd', 'noise_clip': 0.3, 'norm_rewards': 'none', 'norm_states': True, 'num_actors': 1, 'num_minibatches': 32, 'out_dir': '../configs/agents_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan/002', 'policy_activation': 'tanh', 'policy_delay': 2, 'policy_net_type': 'CtsPolicyTD3', 'ppo_epochs': 10, 'ppo_lr': -1, 'ppo_lr_adam': 0.0, 'robust_ppo_beta': 1.0, 'robust_ppo_beta_scheduler_opts': 'same', 'robust_ppo_detach_stdev': False, 'robust_ppo_eps': 0.1, 'robust_ppo_eps_scheduler_opts': 'start=1,length=732', 'robust_ppo_method': 'convex-relax', 'robust_ppo_pgd_steps': 10, 'robust_ppo_reg': 0.1, 'save_frames': False, 'save_frames_path': 'frames/', 'save_iters': 1000, 'save_normalization_factor': False, 'share_weights': False, 'show_env': False, 't': 1000, 'tau': 0.005, 'train_steps': 2400, 'trpo_kl_reduce_func': 'mean', 'use_lstm_val': True, 'val_epochs': 10, 'val_lr': 0.00015, 'value_calc': 'gae', 'value_clipping': True, 'value_multiplier': 0.1, 'value_net_type': 'ValueNet', 'config_path': ['../configs/agent_configs_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan'], 'out_dir_prefix': '../configs/agents_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan', 'num_splits': 1, 'index': 1, 'start': -1, 'end': -1, 'ncpus': 1, 'start_cpuid': 0, 'adv_policy_only': True, 'deterministic': True, 'no_load_adv_policy': False, 'no_smt': False}
Logging in: /gs/bs/tga-mdl/yamabe-mdl/implement/SAIA/configs/agents_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan/002/d54dc8da-3707-4298-af1b-6a70d17355ea
Load normalization factors from normalized_envs/HalfCheetah-v4.pkl
Using activation function ReLU()
Using activation function ReLU()
Using activation function ReLU()
Loading pretrained model models/vanilla_ppo_noob_HalfCheetah-v4.model
Policy runs in deterministic mode. Ignoring Gaussian noise.
Step 0
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -289.768224 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 1000 / 10000
policy lr: [0.000299875], q lr: [0.000299875], disc lr: [9.995833333333334e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.8780241012573242
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------
thread 2 using cpu [2, 194]
disabling policy training (train adversary only)
Running config: {'action_noise_std': 0.1, 'adam_eps': 1e-05, 'adv_adversary_ratio': 1.0, 'adv_adversary_steps': 1, 'adv_clip_eps': 0.2, 'adv_disc_lr': 0.0001, 'adv_entropy_coeff': 1e-05, 'adv_eps': 'same', 'adv_policy_steps': 1, 'adv_ppo_lr_adam': 0.0003, 'adv_q_lr': 'same', 'adv_q_net_type': 'DoubleQNet', 'adv_val_lr': 0.0001, 'advanced_logging': False, 'anneal_lr': True, 'attack_advpolicy_network': 'advpolicy.model', 'attack_eps': 'same', 'attack_method': 'none', 'attack_ratio': 1.0, 'attack_sarsa_action_ratio': 0.5, 'attack_sarsa_network': 'sarsa.model', 'attack_step_eps': 'auto', 'attack_steps': 10, 'batch_size': 256, 'buffer_size': 1000000, 'cg_steps': 10, 'clip_eps': 0.2, 'clip_grad_norm': -1, 'clip_observations': 10.0, 'clip_rewards': 10.0, 'clip_val_eps': 0.2, 'collect_perturbed_states': True, 'cpu': True, 'damping': 0.1, 'demo_episode_size': 20, 'disc_gradient_steps': 1, 'entropy_coeff': 0.0, 'expert_buffer_path': 'demo/Ant-v4_demo.pkl', 'fisher_frac_samples': 0.1, 'force_stop_step': -1, 'game': 'HalfCheetah-v4', 'gamma': 0.99, 'gp_lambda': 10.0, 'gradient_steps': 250, 'history_length': -1, 'initial_std': 1.0, 'initialization': 'orthogonal', 'kl_approximation_iters': -1, 'lambda': 0.95, 'learning_starts': 10000, 'load_model': 'models/vanilla_ppo_noob_HalfCheetah-v4.model', 'log_every': 1, 'max_backtrack': 10, 'max_kl': 0.01, 'max_kl_final': 0.01, 'mode': 'adv_ilfd', 'noise_clip': 0.3, 'norm_rewards': 'none', 'norm_states': True, 'num_actors': 1, 'num_minibatches': 32, 'out_dir': '../configs/agents_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan/000', 'policy_activation': 'tanh', 'policy_delay': 2, 'policy_net_type': 'CtsPolicyTD3', 'ppo_epochs': 10, 'ppo_lr': -1, 'ppo_lr_adam': 0.0, 'robust_ppo_beta': 1.0, 'robust_ppo_beta_scheduler_opts': 'same', 'robust_ppo_detach_stdev': False, 'robust_ppo_eps': 0.03, 'robust_ppo_eps_scheduler_opts': 'start=1,length=732', 'robust_ppo_method': 'convex-relax', 'robust_ppo_pgd_steps': 10, 'robust_ppo_reg': 0.1, 'save_frames': False, 'save_frames_path': 'frames/', 'save_iters': 1000, 'save_normalization_factor': False, 'share_weights': False, 'show_env': False, 't': 1000, 'tau': 0.005, 'train_steps': 2400, 'trpo_kl_reduce_func': 'mean', 'use_lstm_val': True, 'val_epochs': 10, 'val_lr': 0.00015, 'value_calc': 'gae', 'value_clipping': True, 'value_multiplier': 0.1, 'value_net_type': 'ValueNet', 'config_path': ['../configs/agent_configs_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan'], 'out_dir_prefix': '../configs/agents_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan', 'num_splits': 1, 'index': 1, 'start': -1, 'end': -1, 'ncpus': 1, 'start_cpuid': 0, 'adv_policy_only': True, 'deterministic': True, 'no_load_adv_policy': False, 'no_smt': False}
Logging in: /gs/bs/tga-mdl/yamabe-mdl/implement/SAIA/configs/agents_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan/000/ddcf8482-7133-40fc-a654-4b50385e3dcd
Load normalization factors from normalized_envs/HalfCheetah-v4.pkl
Using activation function ReLU()
Using activation function ReLU()
Using activation function ReLU()
Loading pretrained model models/vanilla_ppo_noob_HalfCheetah-v4.model
Policy runs in deterministic mode. Ignoring Gaussian noise.
Step 0
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -286.652300 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 1000 / 10000
policy lr: [0.000299875], q lr: [0.000299875], disc lr: [9.995833333333334e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.8779962062835693
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------
thread 8 using cpu [8, 200]
disabling policy training (train adversary only)
Running config: {'action_noise_std': 0.1, 'adam_eps': 1e-05, 'adv_adversary_ratio': 1.0, 'adv_adversary_steps': 1, 'adv_clip_eps': 0.2, 'adv_disc_lr': 0.0001, 'adv_entropy_coeff': 1e-05, 'adv_eps': 'same', 'adv_policy_steps': 1, 'adv_ppo_lr_adam': 0.0003, 'adv_q_lr': 'same', 'adv_q_net_type': 'DoubleQNet', 'adv_val_lr': 0.0001, 'advanced_logging': False, 'anneal_lr': True, 'attack_advpolicy_network': 'advpolicy.model', 'attack_eps': 'same', 'attack_method': 'none', 'attack_ratio': 1.0, 'attack_sarsa_action_ratio': 0.5, 'attack_sarsa_network': 'sarsa.model', 'attack_step_eps': 'auto', 'attack_steps': 10, 'batch_size': 256, 'buffer_size': 1000000, 'cg_steps': 10, 'clip_eps': 0.2, 'clip_grad_norm': -1, 'clip_observations': 10.0, 'clip_rewards': 10.0, 'clip_val_eps': 0.2, 'collect_perturbed_states': True, 'cpu': True, 'damping': 0.1, 'demo_episode_size': 20, 'disc_gradient_steps': 1, 'entropy_coeff': 0.0, 'expert_buffer_path': 'demo/Ant-v4_demo.pkl', 'fisher_frac_samples': 0.1, 'force_stop_step': -1, 'game': 'HalfCheetah-v4', 'gamma': 0.99, 'gp_lambda': 10.0, 'gradient_steps': 250, 'history_length': -1, 'initial_std': 1.0, 'initialization': 'orthogonal', 'kl_approximation_iters': -1, 'lambda': 0.95, 'learning_starts': 10000, 'load_model': 'models/vanilla_ppo_noob_HalfCheetah-v4.model', 'log_every': 1, 'max_backtrack': 10, 'max_kl': 0.01, 'max_kl_final': 0.01, 'mode': 'adv_ilfd', 'noise_clip': 0.3, 'norm_rewards': 'none', 'norm_states': True, 'num_actors': 1, 'num_minibatches': 32, 'out_dir': '../configs/agents_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan/008', 'policy_activation': 'tanh', 'policy_delay': 2, 'policy_net_type': 'CtsPolicyTD3', 'ppo_epochs': 10, 'ppo_lr': -1, 'ppo_lr_adam': 0.0, 'robust_ppo_beta': 1.0, 'robust_ppo_beta_scheduler_opts': 'same', 'robust_ppo_detach_stdev': False, 'robust_ppo_eps': 0.5, 'robust_ppo_eps_scheduler_opts': 'start=1,length=732', 'robust_ppo_method': 'convex-relax', 'robust_ppo_pgd_steps': 10, 'robust_ppo_reg': 0.1, 'save_frames': False, 'save_frames_path': 'frames/', 'save_iters': 1000, 'save_normalization_factor': False, 'share_weights': False, 'show_env': False, 't': 1000, 'tau': 0.005, 'train_steps': 2400, 'trpo_kl_reduce_func': 'mean', 'use_lstm_val': True, 'val_epochs': 10, 'val_lr': 0.00015, 'value_calc': 'gae', 'value_clipping': True, 'value_multiplier': 0.1, 'value_net_type': 'ValueNet', 'config_path': ['../configs/agent_configs_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan'], 'out_dir_prefix': '../configs/agents_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan', 'num_splits': 1, 'index': 1, 'start': -1, 'end': -1, 'ncpus': 1, 'start_cpuid': 0, 'adv_policy_only': True, 'deterministic': True, 'no_load_adv_policy': False, 'no_smt': False}
Logging in: /gs/bs/tga-mdl/yamabe-mdl/implement/SAIA/configs/agents_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan/008/13537d3e-513f-46f4-abb3-5286cc91683b
Load normalization factors from normalized_envs/HalfCheetah-v4.pkl
Using activation function ReLU()
Using activation function ReLU()
Using activation function ReLU()
Loading pretrained model models/vanilla_ppo_noob_HalfCheetah-v4.model
Policy runs in deterministic mode. Ignoring Gaussian noise.
Step 0
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -269.545917 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 1000 / 10000
policy lr: [0.000299875], q lr: [0.000299875], disc lr: [9.995833333333334e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.8779428005218506
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------
thread 5 using cpu [5, 197]
disabling policy training (train adversary only)
Running config: {'action_noise_std': 0.1, 'adam_eps': 1e-05, 'adv_adversary_ratio': 1.0, 'adv_adversary_steps': 1, 'adv_clip_eps': 0.2, 'adv_disc_lr': 0.0001, 'adv_entropy_coeff': 1e-05, 'adv_eps': 'same', 'adv_policy_steps': 1, 'adv_ppo_lr_adam': 0.0003, 'adv_q_lr': 'same', 'adv_q_net_type': 'DoubleQNet', 'adv_val_lr': 0.0001, 'advanced_logging': False, 'anneal_lr': True, 'attack_advpolicy_network': 'advpolicy.model', 'attack_eps': 'same', 'attack_method': 'none', 'attack_ratio': 1.0, 'attack_sarsa_action_ratio': 0.5, 'attack_sarsa_network': 'sarsa.model', 'attack_step_eps': 'auto', 'attack_steps': 10, 'batch_size': 256, 'buffer_size': 1000000, 'cg_steps': 10, 'clip_eps': 0.2, 'clip_grad_norm': -1, 'clip_observations': 10.0, 'clip_rewards': 10.0, 'clip_val_eps': 0.2, 'collect_perturbed_states': True, 'cpu': True, 'damping': 0.1, 'demo_episode_size': 20, 'disc_gradient_steps': 1, 'entropy_coeff': 0.0, 'expert_buffer_path': 'demo/Ant-v4_demo.pkl', 'fisher_frac_samples': 0.1, 'force_stop_step': -1, 'game': 'HalfCheetah-v4', 'gamma': 0.99, 'gp_lambda': 10.0, 'gradient_steps': 250, 'history_length': -1, 'initial_std': 1.0, 'initialization': 'orthogonal', 'kl_approximation_iters': -1, 'lambda': 0.95, 'learning_starts': 10000, 'load_model': 'models/vanilla_ppo_noob_HalfCheetah-v4.model', 'log_every': 1, 'max_backtrack': 10, 'max_kl': 0.01, 'max_kl_final': 0.01, 'mode': 'adv_ilfd', 'noise_clip': 0.3, 'norm_rewards': 'none', 'norm_states': True, 'num_actors': 1, 'num_minibatches': 32, 'out_dir': '../configs/agents_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan/001', 'policy_activation': 'tanh', 'policy_delay': 2, 'policy_net_type': 'CtsPolicyTD3', 'ppo_epochs': 10, 'ppo_lr': -1, 'ppo_lr_adam': 0.0, 'robust_ppo_beta': 1.0, 'robust_ppo_beta_scheduler_opts': 'same', 'robust_ppo_detach_stdev': False, 'robust_ppo_eps': 0.05, 'robust_ppo_eps_scheduler_opts': 'start=1,length=732', 'robust_ppo_method': 'convex-relax', 'robust_ppo_pgd_steps': 10, 'robust_ppo_reg': 0.1, 'save_frames': False, 'save_frames_path': 'frames/', 'save_iters': 1000, 'save_normalization_factor': False, 'share_weights': False, 'show_env': False, 't': 1000, 'tau': 0.005, 'train_steps': 2400, 'trpo_kl_reduce_func': 'mean', 'use_lstm_val': True, 'val_epochs': 10, 'val_lr': 0.00015, 'value_calc': 'gae', 'value_clipping': True, 'value_multiplier': 0.1, 'value_net_type': 'ValueNet', 'config_path': ['../configs/agent_configs_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan'], 'out_dir_prefix': '../configs/agents_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan', 'num_splits': 1, 'index': 1, 'start': -1, 'end': -1, 'ncpus': 1, 'start_cpuid': 0, 'adv_policy_only': True, 'deterministic': True, 'no_load_adv_policy': False, 'no_smt': False}
Logging in: /gs/bs/tga-mdl/yamabe-mdl/implement/SAIA/configs/agents_attack_vanilla_ppo_noob_HalfCheetah-v4_ilfd_scan/001/30f6ee13-96e6-4d77-b65f-84844964ca30
Load normalization factors from normalized_envs/HalfCheetah-v4.pkl
Using activation function ReLU()
Using activation function ReLU()
Using activation function ReLU()
Loading pretrained model models/vanilla_ppo_noob_HalfCheetah-v4.model
Policy runs in deterministic mode. Ignoring Gaussian noise.
Step 0
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -285.481968 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 1000 / 10000
policy lr: [0.000299875], q lr: [0.000299875], disc lr: [9.995833333333334e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.8778276443481445
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 1
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -285.091426 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 2000 / 10000
policy lr: [0.00029975], q lr: [0.00029975], disc lr: [9.991666666666666e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.40212178230285645
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 1
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -286.190932 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 2000 / 10000
policy lr: [0.00029975], q lr: [0.00029975], disc lr: [9.991666666666666e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.4032175540924072
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 1
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -280.194040 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 2000 / 10000
policy lr: [0.00029975], q lr: [0.00029975], disc lr: [9.991666666666666e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.4034099578857422
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 1
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -281.861202 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 2000 / 10000
policy lr: [0.00029975], q lr: [0.00029975], disc lr: [9.991666666666666e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.40384912490844727
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 1
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -286.136254 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 2000 / 10000
policy lr: [0.00029975], q lr: [0.00029975], disc lr: [9.991666666666666e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.4042541980743408
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 1
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -284.540748 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 2000 / 10000
policy lr: [0.00029975], q lr: [0.00029975], disc lr: [9.991666666666666e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.4040219783782959
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 1
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -281.021995 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 2000 / 10000
policy lr: [0.00029975], q lr: [0.00029975], disc lr: [9.991666666666666e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.4047720432281494
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 1
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -283.657879 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 2000 / 10000
policy lr: [0.00029975], q lr: [0.00029975], disc lr: [9.991666666666666e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.40467047691345215
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 1
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -273.070119 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 2000 / 10000
policy lr: [0.00029975], q lr: [0.00029975], disc lr: [9.991666666666666e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.40647292137145996
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 2
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -284.212521 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 3000 / 10000
policy lr: [0.000299625], q lr: [0.000299625], disc lr: [9.9875e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.3850991725921631
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 2
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -285.853687 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 3000 / 10000
policy lr: [0.000299625], q lr: [0.000299625], disc lr: [9.9875e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.38575148582458496
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 2
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -285.540640 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 3000 / 10000
policy lr: [0.000299625], q lr: [0.000299625], disc lr: [9.9875e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.3845195770263672
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 2
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -285.228141 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 3000 / 10000
policy lr: [0.000299625], q lr: [0.000299625], disc lr: [9.9875e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.38511228561401367
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 2
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -284.193835 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 3000 / 10000
policy lr: [0.000299625], q lr: [0.000299625], disc lr: [9.9875e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.3851301670074463
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 2
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -283.880553 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 3000 / 10000
policy lr: [0.000299625], q lr: [0.000299625], disc lr: [9.9875e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.3860328197479248
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 2
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -286.096622 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 3000 / 10000
policy lr: [0.000299625], q lr: [0.000299625], disc lr: [9.9875e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.3867015838623047
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 2
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -279.735970 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 3000 / 10000
policy lr: [0.000299625], q lr: [0.000299625], disc lr: [9.9875e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.3872826099395752
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 2
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -273.887657 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 3000 / 10000
policy lr: [0.000299625], q lr: [0.000299625], disc lr: [9.9875e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.38840198516845703
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 3
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -282.928202 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 4000 / 10000
policy lr: [0.00029949999999999996], q lr: [0.00029949999999999996], disc lr: [9.983333333333334e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.4037134647369385
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 3
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -282.184894 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 4000 / 10000
policy lr: [0.00029949999999999996], q lr: [0.00029949999999999996], disc lr: [9.983333333333334e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.404435396194458
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 3
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -285.828729 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 4000 / 10000
policy lr: [0.00029949999999999996], q lr: [0.00029949999999999996], disc lr: [9.983333333333334e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.4049966335296631
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 3
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -283.543589 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 4000 / 10000
policy lr: [0.00029949999999999996], q lr: [0.00029949999999999996], disc lr: [9.983333333333334e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.4051074981689453
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 3
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -281.327871 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 4000 / 10000
policy lr: [0.00029949999999999996], q lr: [0.00029949999999999996], disc lr: [9.983333333333334e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.4047384262084961
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 3
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -286.682996 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 4000 / 10000
policy lr: [0.00029949999999999996], q lr: [0.00029949999999999996], disc lr: [9.983333333333334e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.40568041801452637
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 3
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -285.442594 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 4000 / 10000
policy lr: [0.00029949999999999996], q lr: [0.00029949999999999996], disc lr: [9.983333333333334e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.40622758865356445
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 3
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -281.710452 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 4000 / 10000
policy lr: [0.00029949999999999996], q lr: [0.00029949999999999996], disc lr: [9.983333333333334e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.4076576232910156
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 3
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -273.242924 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 4000 / 10000
policy lr: [0.00029949999999999996], q lr: [0.00029949999999999996], disc lr: [9.983333333333334e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.40861010551452637
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 4
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -283.274000 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 5000 / 10000
policy lr: [0.000299375], q lr: [0.000299375], disc lr: [9.979166666666668e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.400484561920166
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 4
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -284.881039 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 5000 / 10000
policy lr: [0.000299375], q lr: [0.000299375], disc lr: [9.979166666666668e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.4016423225402832
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 4
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -284.526122 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 5000 / 10000
policy lr: [0.000299375], q lr: [0.000299375], disc lr: [9.979166666666668e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.40018343925476074
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 4
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -282.879191 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 5000 / 10000
policy lr: [0.000299375], q lr: [0.000299375], disc lr: [9.979166666666668e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.40142297744750977
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 4
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -285.888626 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 5000 / 10000
policy lr: [0.000299375], q lr: [0.000299375], disc lr: [9.979166666666668e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.4011716842651367
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 4
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -282.906820 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 5000 / 10000
policy lr: [0.000299375], q lr: [0.000299375], disc lr: [9.979166666666668e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.40144968032836914
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 4
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -285.245393 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 5000 / 10000
policy lr: [0.000299375], q lr: [0.000299375], disc lr: [9.979166666666668e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.40177226066589355
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 4
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -280.111824 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 5000 / 10000
policy lr: [0.000299375], q lr: [0.000299375], disc lr: [9.979166666666668e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.4030416011810303
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 4
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -276.204194 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 5000 / 10000
policy lr: [0.000299375], q lr: [0.000299375], disc lr: [9.979166666666668e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.4035043716430664
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 5
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -281.573129 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 6000 / 10000
policy lr: [0.00029925], q lr: [0.00029925], disc lr: [9.975000000000001e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.40085601806640625
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 5
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -285.416437 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 6000 / 10000
policy lr: [0.00029925], q lr: [0.00029925], disc lr: [9.975000000000001e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.4005434513092041
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 5
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -287.261629 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 6000 / 10000
policy lr: [0.00029925], q lr: [0.00029925], disc lr: [9.975000000000001e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.40078091621398926
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 5
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -284.119943 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 6000 / 10000
policy lr: [0.00029925], q lr: [0.00029925], disc lr: [9.975000000000001e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.4006814956665039
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 5
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -285.677723 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 6000 / 10000
policy lr: [0.00029925], q lr: [0.00029925], disc lr: [9.975000000000001e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.4013950824737549
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 5
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -286.837351 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 6000 / 10000
policy lr: [0.00029925], q lr: [0.00029925], disc lr: [9.975000000000001e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.4021122455596924
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 5
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -286.332413 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 6000 / 10000
policy lr: [0.00029925], q lr: [0.00029925], disc lr: [9.975000000000001e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.40241146087646484
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 5
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -282.093825 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 6000 / 10000
policy lr: [0.00029925], q lr: [0.00029925], disc lr: [9.975000000000001e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.40477967262268066
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 5
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -281.137841 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 6000 / 10000
policy lr: [0.00029925], q lr: [0.00029925], disc lr: [9.975000000000001e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.40671658515930176
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 6
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -284.937135 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 7000 / 10000
policy lr: [0.00029912499999999997], q lr: [0.00029912499999999997], disc lr: [9.970833333333334e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.40506768226623535
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 6
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -284.710697 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 7000 / 10000
policy lr: [0.00029912499999999997], q lr: [0.00029912499999999997], disc lr: [9.970833333333334e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.40439748764038086
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 6
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -287.395890 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 7000 / 10000
policy lr: [0.00029912499999999997], q lr: [0.00029912499999999997], disc lr: [9.970833333333334e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.40427708625793457
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 6
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -281.827074 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 7000 / 10000
policy lr: [0.00029912499999999997], q lr: [0.00029912499999999997], disc lr: [9.970833333333334e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.4049663543701172
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 6
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -283.757901 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 7000 / 10000
policy lr: [0.00029912499999999997], q lr: [0.00029912499999999997], disc lr: [9.970833333333334e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.4050471782684326
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 6
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -284.693093 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 7000 / 10000
policy lr: [0.00029912499999999997], q lr: [0.00029912499999999997], disc lr: [9.970833333333334e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.40563535690307617
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 6
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -286.996990 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 7000 / 10000
policy lr: [0.00029912499999999997], q lr: [0.00029912499999999997], disc lr: [9.970833333333334e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.40349841117858887
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 6
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -277.915599 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 7000 / 10000
policy lr: [0.00029912499999999997], q lr: [0.00029912499999999997], disc lr: [9.970833333333334e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.40462183952331543
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 6
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -274.604287 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 7000 / 10000
policy lr: [0.00029912499999999997], q lr: [0.00029912499999999997], disc lr: [9.970833333333334e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.404679536819458
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 7
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -283.122137 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 8000 / 10000
policy lr: [0.000299], q lr: [0.000299], disc lr: [9.966666666666667e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.3941161632537842
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 7
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -270.500807 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 8000 / 10000
policy lr: [0.000299], q lr: [0.000299], disc lr: [9.966666666666667e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.39530181884765625
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 7
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -283.275914 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 8000 / 10000
policy lr: [0.000299], q lr: [0.000299], disc lr: [9.966666666666667e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.3956010341644287
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 7
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -283.026129 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 8000 / 10000
policy lr: [0.000299], q lr: [0.000299], disc lr: [9.966666666666667e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.3963816165924072
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 7
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -285.084511 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 8000 / 10000
policy lr: [0.000299], q lr: [0.000299], disc lr: [9.966666666666667e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.3958406448364258
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 7
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -283.891155 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 8000 / 10000
policy lr: [0.000299], q lr: [0.000299], disc lr: [9.966666666666667e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.3960573673248291
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 7
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -284.059643 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 8000 / 10000
policy lr: [0.000299], q lr: [0.000299], disc lr: [9.966666666666667e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.39609766006469727
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 7
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -276.189348 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 8000 / 10000
policy lr: [0.000299], q lr: [0.000299], disc lr: [9.966666666666667e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.39897727966308594
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 7
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -272.739411 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 8000 / 10000
policy lr: [0.000299], q lr: [0.000299], disc lr: [9.966666666666667e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.4005582332611084
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 8
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -282.502727 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 9000 / 10000
policy lr: [0.000298875], q lr: [0.000298875], disc lr: [9.9625e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.41539764404296875
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 8
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -278.456650 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 9000 / 10000
policy lr: [0.000298875], q lr: [0.000298875], disc lr: [9.9625e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.41486549377441406
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 8
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -286.429100 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 9000 / 10000
policy lr: [0.000298875], q lr: [0.000298875], disc lr: [9.9625e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.4153306484222412
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 8
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -277.157778 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 9000 / 10000
policy lr: [0.000298875], q lr: [0.000298875], disc lr: [9.9625e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.41657042503356934
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 8
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -284.162991 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 9000 / 10000
policy lr: [0.000298875], q lr: [0.000298875], disc lr: [9.9625e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.4163854122161865
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 8
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -282.014405 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 9000 / 10000
policy lr: [0.000298875], q lr: [0.000298875], disc lr: [9.9625e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.4165792465209961
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 8
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -285.693942 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 9000 / 10000
policy lr: [0.000298875], q lr: [0.000298875], disc lr: [9.9625e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.4163706302642822
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 8
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -272.948455 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 9000 / 10000
policy lr: [0.000298875], q lr: [0.000298875], disc lr: [9.9625e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.41715073585510254
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 8
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -269.639806 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 9000 / 10000
policy lr: [0.000298875], q lr: [0.000298875], disc lr: [9.9625e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.4172074794769287
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 9
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -282.143518 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 10000 / 10000
policy lr: [0.00029874999999999997], q lr: [0.00029874999999999997], disc lr: [9.958333333333335e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.4065070152282715
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 9
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -283.573946 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 10000 / 10000
policy lr: [0.00029874999999999997], q lr: [0.00029874999999999997], disc lr: [9.958333333333335e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.4069645404815674
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 9
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -285.312441 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 10000 / 10000
policy lr: [0.00029874999999999997], q lr: [0.00029874999999999997], disc lr: [9.958333333333335e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.4069254398345947
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 9
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -283.648219 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 10000 / 10000
policy lr: [0.00029874999999999997], q lr: [0.00029874999999999997], disc lr: [9.958333333333335e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.40721607208251953
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 9
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -286.868067 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 10000 / 10000
policy lr: [0.00029874999999999997], q lr: [0.00029874999999999997], disc lr: [9.958333333333335e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.4074866771697998
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 9
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -284.102478 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 10000 / 10000
policy lr: [0.00029874999999999997], q lr: [0.00029874999999999997], disc lr: [9.958333333333335e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.40847015380859375
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 9
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -287.121329 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 10000 / 10000
policy lr: [0.00029874999999999997], q lr: [0.00029874999999999997], disc lr: [9.958333333333335e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.4071519374847412
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 9
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -278.200822 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 10000 / 10000
policy lr: [0.00029874999999999997], q lr: [0.00029874999999999997], disc lr: [9.958333333333335e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.40816521644592285
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------

Step 9
skipping policy training because learning rate is 0. adv_policy_steps and adv_adversary_steps ignored.
++++++++ Adversary training ++++++++++
Current mean reward: -272.445789 | mean episode length: 1000.000000
Skipping policy training because number of experiences is less than learning_starts: 10000 / 10000
policy lr: [0.00029874999999999997], q lr: [0.00029874999999999997], disc lr: [9.958333333333335e-05]
Policy Loss: 0, | Q Loss: 0, | Disc Loss: 0
Time elapsed (s): 0.4107975959777832
Agent stdevs: 1.1051711
--------------------------------------------------------------------------------
